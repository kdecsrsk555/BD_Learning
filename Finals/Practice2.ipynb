{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11a6d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#필요한 라이브러리 호출 공간\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46756088",
   "metadata": {},
   "source": [
    "데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "file_name = \"\"\n",
    "def reading_csv(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    print(df.shape)\n",
    "    print(df.head())\n",
    "    return df\n",
    "\n",
    "df = reading_csv(file_name)\n",
    "\n",
    "# 결측치 확인\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# 결측치 처리 (평균 or 최빈값 or 중위값)\n",
    "si = SimpleImputer(strategy=\"mean\")\n",
    "ss = StandardScaler()\n",
    "le = LabelEncoder()\n",
    "oe = OneHotEncoder(sparse=False)\n",
    "num_col = []\n",
    "cat_col = []\n",
    "\n",
    "def processing(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # 수치형 컬럼 처리\n",
    "    df[num_col] = si.fit_transform(df[num_col])\n",
    "    df['파생변수'] = pd.cut(df['col'], bins=[0, 18, 30, 50, 100], labels=['Teen', 'Young', 'Adults', 'Senior'])\n",
    "    df[num_col] = ss.fit_transform(df[num_col])\n",
    "\n",
    "    # 범주형 컬럼 처리\n",
    "    for col in cat_col:# label encoding\n",
    "            df[col] = le.fit_transform(df[col])\n",
    "\n",
    "    return df\n",
    "\n",
    "processed_df = processing(df)\n",
    "print(processed_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523ee0ca",
   "metadata": {},
   "source": [
    "Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14852af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 타겟 변수 지정\n",
    "target_col = 'target'  # 실제 변수명으로 변경\n",
    "\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "# 훈련/검증 데이터 분할 (8:2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee432c46",
   "metadata": {},
   "source": [
    "모델 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aa9909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 적용 및 성능 평가\n",
    "from sklearn import linear_model  \n",
    "# 로지스틱 회귀 (이진 분류에 특화)\n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "# 앙상블 모델: 여러 결정트리를 결합하여 성능 향상\n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "# 단순 신경망 기반의 선형 분류기\n",
    "from sklearn.linear_model import Perceptron  \n",
    "# 확률적 경사하강법 기반 선형 분류기\n",
    "from sklearn.linear_model import SGDClassifier  \n",
    "# 결정 트리 기반 분류기 (if-else 형태의 트리 구조)\n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "# K-최근접 이웃 알고리즘 (거리 기반 분류)\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "# 서포트 벡터 머신 (복잡한 경계 분류, 마진 기반)\n",
    "# LinearSVC: 선형 SVM (SVM의 일종, 대용량에 빠르고 선형 분리에 적합)\n",
    "from sklearn.svm import SVC, LinearSVC  \n",
    "# 확률 기반 분류기 (조건부 독립 가정)\n",
    "from sklearn.naive_bayes import GaussianNB  \n",
    "\n",
    "# 🔹 [3] 시각화 도구 임포트\n",
    "import seaborn as sns  \n",
    "%matplotlib inline  \n",
    "from matplotlib import pyplot as plt  \n",
    "from matplotlib import style  # 그래프 스타일 지정용\n",
    "\n",
    "# 의사결정 나무 --> 복자도가 높아지면 일반화 성능이 떨어짐, 과적합 가능성\n",
    "decision_tree=DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train, y_train)\n",
    "Y_pred=decision_tree.predict(X_test)\n",
    "train_acc_decision_tree=round(decision_tree.score(X_train, y_train)*100, 2)\n",
    "test_acc_decision_tree=round(decision_tree.score(X_test, y_test)*100, 2)\n",
    "train_acc_decision_tree,test_acc_decision_tree\n",
    "\n",
    "\n",
    "# 랜덤 포레스트 --> 과적합 방지에 강함, 하지만 하이퍼파라미터 기본값 사용 등으로 일반화 성능이 크게 향상되지 않음\n",
    "# max_depth, min_sample_leaf 제한\n",
    "# n_estimator 증가\n",
    "# GridSearchCV 사용 --> 최적의 하이퍼 파라미터 조건 탐색\n",
    "random_forest=RandomForestClassifier(n_estimators=100)\n",
    "random_forest.fit(X_train, y_train)\n",
    "Y_prediction=random_forest.predict(X_test)\n",
    "random_forest.score(X_train, y_train)\n",
    "train_acc_random_forest=round(random_forest.score(X_train, y_train)*100, 2)\n",
    "test_acc_random_forest=round(random_forest.score(X_test, y_test)*100, 2)\n",
    "train_acc_random_forest,test_acc_random_forest\n",
    "\n",
    "\n",
    "# 로지스틱 회귀 --> 과적합 위험 낮음, 특히 특성이 별로 없을 때\n",
    "log_reg=LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "Y_pred=log_reg.predict(X_test)\n",
    "train_acc_log=round(log_reg.score(X_train, y_train)*100, 2)\n",
    "test_acc_log=round(log_reg.score(X_test, y_test)*100, 2)\n",
    "train_acc_log, test_acc_log\n",
    "\n",
    "\n",
    "# 나이브 베이즈 --> 로지스틱 회귀보다 성능 낮음\n",
    "gaussian=GaussianNB()\n",
    "gaussian.fit(X_train, y_train)\n",
    "Y_pred=gaussian.predict(X_test)\n",
    "train_acc_gaussian=round(gaussian.score(X_train, y_train)*100, 2)\n",
    "test_acc_gaussian=round(gaussian.score(X_test, y_test)*100, 2)\n",
    "train_acc_gaussian,test_acc_gaussian\n",
    "\n",
    "# Linear SVC\n",
    "svc=LinearSVC()\n",
    "svc.fit(X_train, y_train)\n",
    "Y_pred=svc.predict(X_test)\n",
    "train_acc_svc=round(svc.score(X_train, y_train)*100, 2)\n",
    "test_acc_svc=round(svc.score(X_test, y_test)*100, 2)\n",
    "train_acc_svc,test_acc_svc\n",
    "\n",
    "# Perceptron\n",
    "perceptron=Perceptron(max_iter=5)\n",
    "perceptron.fit(X_train, y_train)\n",
    "Y_pred=perceptron.predict(X_test)\n",
    "train_acc_perceptron=round(perceptron.score(X_train, y_train)*100, 2)\n",
    "test_acc_perceptron=round(perceptron.score(X_test, y_test)*100, 2)\n",
    "train_acc_perceptron,test_acc_perceptron\n",
    "\n",
    "\n",
    "#SDGclassifier --> 과소적합 (max_iter 늘려야됨)\n",
    "sgd=linear_model.SGDClassifier(max_iter=5, tol=None)\n",
    "sgd.fit(X_train, y_train)\n",
    "Y_pred=sgd.predict(X_test)\n",
    "sgd.score(X_train, y_train)\n",
    "train_acc_sgd=round(sgd.score(X_train, y_train)*100, 2)\n",
    "test_acc_sgd=round(sgd.score(X_test, y_test)*100, 2)\n",
    "train_acc_sgd,test_acc_sgd\n",
    "\n",
    "\n",
    "#모델 비교 방식\n",
    "results = pd.DataFrame({\n",
    " 'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', 'Random Forest','Naive Bayes', 'Perceptron', 'Stochastic Gradient Decent', 'Decision Tree'],\n",
    " 'train_Score': [train_acc_svc, train_acc_knn, train_acc_log,train_acc_random_forest,\n",
    "train_acc_gaussian, train_acc_perceptron, train_acc_sgd, train_acc_decision_tree],\n",
    " 'test_Score': [test_acc_svc, test_acc_knn,test_acc_log, test_acc_random_forest, test_acc_gaussian, test_acc_perceptron, test_acc_sgd, test_acc_decision_tree]})\n",
    "result_df=results.sort_values(by='train_Score', ascending=False)\n",
    "result_df=result_df.set_index('Model')\n",
    "result_df.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
