{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37f2768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 필요한 라이브러리 임포트\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "import torch\n",
    "\n",
    "# ✅ 1. IMDB 영화 리뷰 데이터셋 로드\n",
    "# IMDB는 'train'과 'test'로 나뉜 binary classification용 감성 데이터셋\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# ✅ 2. 과적합 방지를 위한 균형잡힌 샘플링\n",
    "# 아래 방식으로 데이터 훈련/테스트 세트 선택 할 경우 데이터 편향으로 인한 과적합 또는 오적합 발생\n",
    "# train_data = dataset[\"train\"].select(range(2000))  # 메모리 초과 방지용\n",
    "# test_data = dataset[\"test\"].select(range(500))     # 메모리 초과 방지용\n",
    "# 원본 train 데이터는 부정(0) 리뷰가 앞쪽에 몰려 있음 ⇒ 모델 편향 유발\n",
    "# 따라서 부정/긍정 각각 5000개씩 무작위 추출하여 균형 유지\n",
    "neg_samples = dataset[\"train\"].filter(lambda x: x['label'] == 0).shuffle(seed=42).select(range(5000))\n",
    "pos_samples = dataset[\"train\"].filter(lambda x: x['label'] == 1).shuffle(seed=42).select(range(5000))\n",
    "\n",
    "# 부정/긍정 결합 후 다시 셔플하여 train 데이터 구성\n",
    "balanced_dataset = concatenate_datasets([neg_samples, pos_samples]).shuffle(seed=42)\n",
    "\n",
    "# ✅ 3. 학습/평가용 데이터 분리\n",
    "# 학습 데이터: 균형 샘플 중 2,000개 사용 (Colab 환경 고려)\n",
    "# 테스트 데이터: 원본 IMDB test 데이터 중 500개 사용\n",
    "train_data = balanced_dataset.select(range(2000))\n",
    "test_data = load_dataset(\"imdb\")[\"test\"].select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23eb097e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 500\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09434b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 데이터 로딩\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "len(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61b66eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism\n",
      "comp.graphics\n",
      "comp.os.ms-windows.misc\n",
      "comp.sys.ibm.pc.hardware\n",
      "comp.sys.mac.hardware\n",
      "comp.windows.x\n",
      "misc.forsale\n",
      "rec.autos\n",
      "rec.motorcycles\n",
      "rec.sport.baseball\n",
      "rec.sport.hockey\n",
      "sci.crypt\n",
      "sci.electronics\n",
      "sci.med\n",
      "sci.space\n",
      "soc.religion.christian\n",
      "talk.politics.guns\n",
      "talk.politics.mideast\n",
      "talk.politics.misc\n",
      "talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "for a in newsgroups_test.target_names:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2cabda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✂️ 훈련 데이터셋에서 리뷰 텍스트와 라벨만 분리하여 별도 변수에 저장\n",
    "train_texts = train_data[\"text\"]   # 리뷰 본문 텍스트 목록 (리스트 형태)\n",
    "train_labels = train_data[\"label\"] # 감성 레이블 (0: 부정, 1: 긍정)\n",
    "\n",
    "# 🔤 사전학습된 BERT용 토크나이저 로드 (WordPiece 기반 토큰화)\n",
    "# 'bert-base-uncased'는 모든 텍스트를 소문자로 처리함\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# ✂️ 최대 시퀀스 길이 설정 (너무 긴 문장은 자르고, 짧은 문장은 패딩함)\n",
    "MAX_LEN = 256\n",
    "\n",
    "# 🧪 토크나이저로 문장들을 BERT 입력 형태로 변환\n",
    "# truncation=True → 길면 자르기 / padding='max_length' → 짧으면 256길이에 맞춰 패딩\n",
    "# return_tensors=\"pt\" → PyTorch 텐서 형태로 반환\n",
    "encodings = tokenizer(\n",
    "    train_texts, \n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=MAX_LEN,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# 🔠 인코딩 결과에서 input_ids와 attention_mask 추출\n",
    "# input_ids: 토큰의 정수 인덱스\n",
    "# attention_mask: 패딩이 아닌 부분은 1, 패딩된 부분은 0\n",
    "train_inputs = encodings[\"input_ids\"]\n",
    "train_masks = encodings[\"attention_mask\"]\n",
    "\n",
    "# 🏷️ 라벨도 torch 텐서 형태로 변환 (모델 학습에 사용하기 위함)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6157617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧳 PyTorch Dataset 및 Dataloader 구성\n",
    "\n",
    "# 한 번에 학습할 데이터 수 설정 (미니 배치 크기)\n",
    "batch_size = 16\n",
    "\n",
    "# 📦 TensorDataset: 입력 데이터(input_ids), 마스크(attention_mask), 정답 라벨(labels)을 하나의 Dataset으로 묶음\n",
    "# => 학습 시 각 인덱스에 대해 (input_ids, attention_mask, label)을 반환\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "\n",
    "# 🚚 DataLoader: 실제 학습에서 배치 단위로 데이터를 공급해주는 역할\n",
    "# RandomSampler: 매 epoch마다 데이터를 **무작위로 섞어** 샘플링 → 학습 성능 향상에 도움\n",
    "# batch_size: 한 배치에 포함될 샘플 수\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,                     # 학습할 데이터셋\n",
    "    sampler=RandomSampler(train_dataset),  # 랜덤 샘플링 (epoch마다 데이터 순서 섞기)\n",
    "    batch_size=batch_size             # 한 번에 읽어올 샘플 개수 (배치 단위)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27acfbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 🔧 GPU 사용 여부 확인\n",
    "# CUDA가 사용 가능하면 GPU 사용, 그렇지 않으면 CPU 사용\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 🧠 사전학습된 BERT 분류 모델 로딩\n",
    "# 'bert-base-uncased': 소문자화된 BERT 사전학습 모델\n",
    "# num_labels=2: 이진 분류 문제 (긍정/부정)\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# 모델을 선택한 장치(GPU 또는 CPU)로 이동\n",
    "model.to(device)\n",
    "\n",
    "# 🔁 옵티마이저 및 학습률 스케줄러 설정\n",
    "# AdamW: BERT 논문에서 권장하는 옵티마이저 (가중치 감쇠 적용)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "# 총 학습 epoch 수 설정\n",
    "epochs = 2\n",
    "\n",
    "# 전체 학습 스텝 수 계산 (총 배치 수 × epoch 수)\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# 학습률 스케줄러 설정\n",
    "# Warm-up 없이 선형 감소 방식으로 학습률 조정\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,                 # 사용할 옵티마이저\n",
    "    num_warmup_steps=0,        # warm-up 단계 없음\n",
    "    num_training_steps=total_steps  # 전체 학습 스텝 수\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdf96d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "  ▶ Step 0/125 - Elapsed: 0:00:00\n",
      "  ▶ Step 100/125 - Elapsed: 0:20:38\n",
      "✅ Epoch 1 Avg Loss: 0.5036\n",
      "🕒 Epoch time: 0:25:37\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "  ▶ Step 0/125 - Elapsed: 0:00:00\n",
      "  ▶ Step 100/125 - Elapsed: 0:18:21\n",
      "✅ Epoch 2 Avg Loss: 0.2471\n",
      "🕒 Epoch time: 0:23:10\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 🧬 랜덤 시드 고정 (결과 재현 가능성을 높이기 위함)\n",
    "seed_val = 777\n",
    "random.seed(seed_val)                    # 파이썬 random 고정\n",
    "np.random.seed(seed_val)                # 넘파이 random 고정\n",
    "torch.manual_seed(seed_val)             # PyTorch random 고정\n",
    "torch.cuda.manual_seed_all(seed_val)    # CUDA에서도 동일하게 시드 고정\n",
    "\n",
    "# 학습 시간 포맷 함수 (초 → 시:분:초)\n",
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round(elapsed))))\n",
    "\n",
    "# 🏋️ 모델 학습 시작\n",
    "model.train()  # 모델을 학습 모드로 전환 (Dropout, BatchNorm 등이 학습 모드로 작동)\n",
    "\n",
    "# 각 epoch 반복\n",
    "for epoch_i in range(epochs):\n",
    "    print(f\"\\n======== Epoch {epoch_i + 1} / {epochs} ========\")\n",
    "    t0 = time.time()  # 학습 시작 시간 저장\n",
    "    total_loss = 0    # 누적 손실 초기화\n",
    "\n",
    "    # 미니배치 반복\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # 매 100스텝마다 경과 시간 출력\n",
    "        if step % 100 == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print(f\"  ▶ Step {step}/{len(train_dataloader)} - Elapsed: {elapsed}\")\n",
    "\n",
    "        # 입력 데이터와 라벨을 장치(GPU/CPU)로 이동\n",
    "        b_input_ids, b_input_mask, b_labels = [b.to(device) for b in batch]\n",
    "\n",
    "        model.zero_grad()  # 기울기 초기화\n",
    "\n",
    "        # 🔁 순전파 → 손실 계산\n",
    "        outputs = model(\n",
    "            b_input_ids, \n",
    "            attention_mask=b_input_mask, \n",
    "            labels=b_labels\n",
    "        )\n",
    "        loss = outputs.loss         # CrossEntropyLoss 포함\n",
    "        total_loss += loss.item()   # 손실 누적\n",
    "\n",
    "        # 🔄 역전파 → 가중치 갱신\n",
    "        loss.backward()  # 손실 기준으로 역전파 수행\n",
    "\n",
    "        # 그래디언트 클리핑: exploding gradient 방지\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()  # 가중치 업데이트\n",
    "        scheduler.step()  # 학습률 조정\n",
    "\n",
    "    # ⏱ 에폭 단위 평균 손실 출력\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"✅ Epoch {epoch_i+1} Avg Loss: {avg_loss:.4f}\")\n",
    "    print(f\"🕒 Epoch time: {format_time(time.time() - t0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "294043d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "c:\\Users\\kdecs\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 입력 문장: This movie was absolutely fantastic. I loved the plot and the characters.\n",
      "  LABEL_0 → 0.0417\n",
      "  LABEL_1 → 0.9583\n",
      "\n",
      "📌 입력 문장: It was the worst film I have ever seen. Completely boring.\n",
      "  LABEL_0 → 0.9469\n",
      "  LABEL_1 → 0.0531\n",
      "\n",
      "📌 입력 문장: Not bad, but not great either. Just okay.\n",
      "  LABEL_0 → 0.8529\n",
      "  LABEL_1 → 0.1471\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 📍 GPU 사용 가능 여부 확인 → GPU가 있으면 0, 없으면 CPU(-1)로 설정\n",
    "device_id = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# 🤖 학습된 모델로 파이프라인 구성\n",
    "clf_pipeline = pipeline(\n",
    "    task=\"text-classification\",        # 작업 유형: 문장 분류\n",
    "    model=model,                       # fine-tuning된 BERT 분류 모델\n",
    "    tokenizer=tokenizer,               # 같은 base의 토크나이저 사용\n",
    "    device=device_id,                  # 디바이스 설정: GPU or CPU\n",
    "    return_all_scores=True,            # 두 클래스의 확률 모두 출력\n",
    "    function_to_apply=\"softmax\"        # 출력 로짓 → 확률값으로 변환\n",
    ")\n",
    "\n",
    "# 🔍 예측할 테스트 문장 리스트\n",
    "sample_texts = [\n",
    "    \"This movie was absolutely fantastic. I loved the plot and the characters.\",   # 긍정\n",
    "    \"It was the worst film I have ever seen. Completely boring.\",                  # 부정\n",
    "    \"Not bad, but not great either. Just okay.\"                                    # 중립에 가까움 (판단 애매)\n",
    "]\n",
    "\n",
    "# 🧾 파이프라인을 통해 예측 수행\n",
    "predictions = clf_pipeline(sample_texts)\n",
    "\n",
    "# 📋 각 문장별 예측 결과 출력\n",
    "for text, result in zip(sample_texts, predictions):\n",
    "    print(f\"\\n📌 입력 문장: {text}\")\n",
    "    for label in result:\n",
    "        print(f\"  {label['label']} → {label['score']:.4f}\")  # label: POSITIVE/NEGATIVE, score: 확률값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e56d91aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Test Accuracy: 0.8920\n",
      "✅ Test F1 Score: 0.9429\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "\n",
    "# 🎯 테스트셋 전처리\n",
    "test_texts = test_data[\"text\"]             # 테스트 문장 리스트\n",
    "test_labels = test_data[\"label\"]           # 테스트 레이블 리스트\n",
    "\n",
    "# 🧪 토크나이징 및 인코딩 (훈련과 동일하게 max_length 기준 padding/truncation 수행)\n",
    "test_encodings = tokenizer(\n",
    "    test_texts,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=MAX_LEN,\n",
    "    return_tensors=\"pt\"                    # PyTorch 텐서 형태로 반환\n",
    ")\n",
    "\n",
    "test_inputs = test_encodings[\"input_ids\"]         # 입력 토큰 ID\n",
    "test_masks = test_encodings[\"attention_mask\"]     # 어텐션 마스크\n",
    "test_labels_tensor = torch.tensor(test_labels)    # 정답 레이블\n",
    "\n",
    "# 📦 PyTorch용 테스트 Dataset 및 DataLoader 구성\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels_tensor)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)\n",
    "\n",
    "# 📈 평가 함수 정의\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()              # 평가 모드 (Dropout/BatchNorm 등 비활성화)\n",
    "    preds = []                # 예측값 저장용 리스트\n",
    "    true_labels = []          # 실제값 저장용 리스트\n",
    "\n",
    "    for batch in dataloader:\n",
    "        # 미니배치 데이터를 GPU/CPU로 이동\n",
    "        b_input_ids, b_input_mask, b_labels = [b.to(device) for b in batch]\n",
    "        \n",
    "        with torch.no_grad():  # 그래디언트 계산 비활성화 (속도↑, 메모리↓)\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1).cpu().numpy()  # 가장 확률 높은 클래스 선택\n",
    "        labels = b_labels.cpu().numpy()\n",
    "\n",
    "        preds.extend(predictions)      # 예측 결과 누적\n",
    "        true_labels.extend(labels)     # 실제 레이블 누적\n",
    "\n",
    "    return preds, true_labels\n",
    "\n",
    "# 🧪 평가 실행\n",
    "preds, true_labels = evaluate_model(model, test_dataloader)\n",
    "\n",
    "# ✅ 정확도 및 F1 점수 계산 및 출력\n",
    "acc = accuracy_score(true_labels, preds)                         # 정확도: 예측과 실제 일치 비율\n",
    "f1 = f1_score(true_labels, preds, average='weighted')            # F1 점수: 정밀도와 재현율의 조화 평균\n",
    "\n",
    "print(f\"\\n✅ Test Accuracy: {acc:.4f}\")\n",
    "print(f\"✅ Test F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8346966b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 앞 10,000개만 선택\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m train_subset \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10000\u001b[39m))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 레이블 리스트 추출\u001b[39;00m\n\u001b[0;32m      7\u001b[0m labels \u001b[38;5;241m=\u001b[39m train_subset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 앞 10,000개만 선택\n",
    "train_subset = dataset[\"train\"].select(range(10000))\n",
    "\n",
    "# 레이블 리스트 추출\n",
    "labels = train_subset[\"label\"]\n",
    "\n",
    "# 레이블 분포 확인\n",
    "label_count = Counter(labels)\n",
    "print(label_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c383ab76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
